# -*- coding: utf-8 -*-
"""Medical Diagnosis_BioBERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EFoqt7j034O0OHNy8uNpOCsW7amcp4lU
"""

!kaggle datasets download -d ayeshacamran/mimic-clinical-notes

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

import zipfile
zip_ref =zipfile.ZipFile('/content/mimic-clinical-notes.zip','r');
zip_ref.extractall('/content');
zip_ref.close();

!pip install transformers datasets scikit-learn torch

import pandas as pd


noteevents_path = '/content/NOTEEVENTS.csv/NOTEEVENTS.csv'
notes_df = pd.read_csv(noteevents_path)
sample_data = notes_df.sample(n=5000, random_state=42);


print(sample_data.head())
print(sample_data.info())

print(len(sample_data))

!pip install tensorflow

print(sample_data.head());

print(notes_df['TEXT'].head());

!pip install nltk pandas scikit-learn

import nltk
nltk.download('punkt')

#Punkt helps segment clinical text into sentences, improving the quality of your text preprocessing for tasks like NER or diagnosis classification.

from nltk.tokenize import word_tokenize

import pandas as pd
import nltk
import re



def preprocess_text(text):
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = text.lower().strip()
    return text

sample_data['clean_text'] = sample_data['TEXT'].apply(preprocess_text)

sample_data['clean_text'].head()

disease_dict = {
    'pneumonia', 'fracture', 'tumor', 'arthritis', 'scoliosis',
    'subarachnoid hemorrhage', 'diabetes', 'hypertension', 'asthma',
    'heart disease', 'stroke', 'neoplasm', 'metastasis', 'lesion',
    'cyst', 'growth', 'osteomyelitis', 'disc herniation',
    'spondylolisthesis', 'degenerative disc disease', 'osteoporosis',
    'aneurysm', 'thrombosis', 'embolism', 'spinal stenosis',
    'spinal cord injury', 'sciatica', 'abscess', 'bacterial infection',
    'viral infection','fractures'
}

symptom_dict = {
    'pain', 'swelling', 'inflammation', 'discomfort', 'back pain',
    'fever', 'nausea', 'fatigue', 'dizziness', 'headache', 'cough',
    'chest pain', 'shortness of breath', 'radiating pain', 'sharp pain',
    'dull pain', 'localized pain', 'weakness', 'numbness', 'tingling',
    'edema', 'bruising', 'deformity', 'compression', 'subluxation',
    'fracture line'
}

# Updated Treatment Dictionary
treatment_dict = {
    'surgery', 'radiation', 'medication', 'physical therapy', 'mri',
    'antibiotics', 'analgesics', 'chemotherapy', 'immunotherapy',
    'psychotherapy', 'catheterization', 'endoscopy', 'biopsy',
    'surgical decompression', 'joint replacement', 'arthroscopy',
    'spinal fusion', 'contrast enhancement', 'ct scan', 'x-ray',
    'pet scan', 'ultrasound', 'pain management', 'anti-inflammatory drugs',
    'anticoagulants', 'steroid injections', 'bracing', 'orthotics',
    'rehabilitation therapy', 'pain relief techniques'
}



def custom_ner(text):
    tokens = word_tokenize(text)
    labels = []
    for token in tokens:
        if token.lower() in disease_dict:
            labels.append((token, 'DISEASE'))
        elif token.lower() in symptom_dict:
            labels.append((token, 'SYMPTOM'))
        elif token.lower() in treatment_dict:
            labels.append((token, 'TREATMENT'))
        else:
            labels.append((token, 'O'))
    return labels



sample_data['custom_ner'] = sample_data['clean_text'].apply(custom_ner)

all_data_ner = sample_data.copy()

imaging_data = sample_data[sample_data['clean_text'].str.contains('mri|x-ray|radiograph|ct scan', case=False)]

for idx, row in imaging_data.iterrows():
    print(f"Annotated Text: {row['custom_ner']}\n")
    print("-" * 50)
    if idx >= 5:
        break

print(imaging_data['custom_ner'])

import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments
from transformers import DataCollatorForTokenClassification
from datasets import Dataset, DatasetDict

words = []
labels = []
for row in imaging_data['custom_ner']:
  for word, label in row:
    words.append(word)
    labels.append(label)

df = pd.DataFrame({
    'Word': words,
    'Label': labels
})


df.to_csv('annotations.csv', index=False)

import pandas as pd


df = pd.read_csv('annotations.csv')

print(df.head())
print(df.count())

from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments
from transformers import DataCollatorForTokenClassification
from datasets import Dataset, DatasetDict


labels = df['Label'].unique()
label2id = {label: id for id, label in enumerate(labels)}
id2label = {id: label for label, id in label2id.items()}


tokenizer = BertTokenizer.from_pretrained("dmis-lab/biobert-base-cased-v1.1")

def encode_examples(examples):

    encoded = tokenizer(examples["Word"], truncation=True, padding="max_length", max_length=128)

    encoded_labels = []
    for label in examples["Label"]:
        if isinstance(label, list):
            encoded_label = [label2id[l] for l in label]
        else:
            encoded_label = [label2id[label]]
        encoded_labels.append(encoded_label)

    encoded["labels"] = encoded_labels
    return encoded


dataset = Dataset.from_pandas(df)

dataset = dataset.map(encode_examples, batched=True)

train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)


dataset_dict = DatasetDict({
    "train": train_dataset,
    "test": test_dataset
})


data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)

df = df.reset_index(drop=True)


dataset = Dataset.from_pandas(df)

# Continue with encoding and splitting
dataset = dataset.map(encode_examples, batched=True)
train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)


dataset_dict = DatasetDict({
    "train": Dataset.from_dict(train_dataset),
    "test": Dataset.from_dict(test_dataset)
})

model = BertForTokenClassification.from_pretrained(
    "dmis-lab/biobert-base-cased-v1.1",
    num_labels=len(labels),
    id2label=id2label,
    label2id=label2id
)

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    num_train_epochs=8,
    weight_decay=0.01,
    gradient_checkpointing=False,  # Ensure gradient checkpointing is off if not needed
)

# Create the Trainer instance
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset_dict["train"],
    eval_dataset=dataset_dict["test"],
    data_collator=data_collator,
)

trainer.train()

from transformers import pipeline


generator = pipeline('text-generation', model='gpt2', truncation=True)

def analyze_medical_report(report):
    prompt = f"""
    Please extract the following details from the medical report:
    - Disease(s)
    - Medication(s)
    - Symptom(s)
    - Treatment(s)

    Report: "{report}"

    Format the response as:
    - Diseases: ...
    - Medications: ...
    - Symptoms: ...
    - Treatments: ...
    """

    result = generator(prompt, max_new_tokens=100, num_return_sequences=1)[0]['generated_text']

    start_idx = result.find("- Diseases:")
    end_idx = result.find("Case", start_idx)

    if start_idx != -1 and end_idx != -1:
        result = result[start_idx:end_idx].strip()
    else:
        result = result[start_idx:].strip() if start_idx != -1 else "Unable to extract details."

    return result


medical_report = """
The patient has been experiencing frequent urination, fatigue, and increased thirst.
They have been diagnosed with Type 2 diabetes and are currently on Metformin.
The doctor has recommended lifestyle changes, including diet and exercise, as part of the treatment plan.
"""

output = analyze_medical_report(medical_report)
print(output)